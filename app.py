# -*- coding: utf-8 -*-
"""English to French Translation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OFq61gs_DzqTSii-cvPZt75vh0w1i1ak
"""

!pip install tensorflow numpy pandas

!wget http://www.manythings.org/anki/fra-eng.zip
!unzip fra-eng.zip

# Read the dataset
lines = open('fra.txt', encoding='utf-8').read().strip().split('\n')
pairs = [line.split('\t') for line in lines]

# Download and extract English–French sentence pairs
!wget http://www.manythings.org/anki/fra-eng.zip
!unzip fra-eng.zip

# Load and preview the dataset
lines = open('fra.txt', encoding='utf-8').read().strip().split('\n')
pairs = [line.split('\t') for line in lines]
print(f"Total pairs: {len(pairs)}")
print("Example:", pairs[0])

import tensorflow as tf
import numpy as np
import re

# Function to clean each sentence
def preprocess_sentence(s):
    s = s.lower().strip()
    s = re.sub(r"([?.!,¿])", r" \1 ", s)
    s = re.sub(r'[" "]+', " ", s)
    s = re.sub(r"[^a-zA-Z?.!,¿]+", " ", s)
    s = s.strip()
    s = '<start> ' + s + ' <end>'
    return s

# Limit to 30,000 examples
num_examples = 30000
input_texts = []
target_texts = []

# Safe unpacking of sentence pairs
for pair in pairs[:num_examples]:
    if len(pair) < 2:
        continue  # skip malformed lines
    eng, fr = pair[0], pair[1]
    input_texts.append(preprocess_sentence(eng))
    target_texts.append(preprocess_sentence(fr))

# Preview preprocessed sentences
print("Total cleaned pairs:", len(input_texts))
print("Sample input sentence:", input_texts[0])
print("Sample target sentence:", target_texts[0])

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Tokenize input (English)
inp_tokenizer = Tokenizer(filters='')
inp_tokenizer.fit_on_texts(input_texts)
input_tensor = inp_tokenizer.texts_to_sequences(input_texts)
input_tensor = pad_sequences(input_tensor, padding='post')

# Tokenize target (French)
tgt_tokenizer = Tokenizer(filters='')
tgt_tokenizer.fit_on_texts(target_texts)
target_tensor = tgt_tokenizer.texts_to_sequences(target_texts)
target_tensor = pad_sequences(target_tensor, padding='post')

# Vocabulary sizes
inp_vocab_size = len(inp_tokenizer.word_index) + 1
tgt_vocab_size = len(tgt_tokenizer.word_index) + 1

# Print sample tokenized sentence
print("Input tensor example:", input_tensor[0])
print("Target tensor example:", target_tensor[0])
print("Input vocab size:", inp_vocab_size)
print("Target vocab size:", tgt_vocab_size)

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Embedding, Dense

# Set embedding and units
embedding_dim = 256
units = 512

# Encoder
encoder_inputs = Input(shape=(None,))
enc_emb = Embedding(input_dim=inp_vocab_size, output_dim=embedding_dim)(encoder_inputs)
encoder_lstm, state_h, state_c = LSTM(units, return_state=True)(enc_emb)
encoder_states = [state_h, state_c]

# Decoder
decoder_inputs = Input(shape=(None,))
dec_emb_layer = Embedding(input_dim=tgt_vocab_size, output_dim=embedding_dim)
dec_emb = dec_emb_layer(decoder_inputs)

decoder_lstm = LSTM(units, return_sequences=True, return_state=True)
decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)

decoder_dense = Dense(tgt_vocab_size, activation='softmax')
decoder_outputs = decoder_dense(decoder_outputs)

# Define the model
model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')
model.summary()

# Convert target tensor to numpy array
import numpy as np

decoder_input_data = target_tensor[:, :-1]  # all except last token
decoder_target_data = target_tensor[:, 1:]  # all except first token

# Add an extra dimension (required for sparse_categorical_crossentropy)
decoder_target_data = np.expand_dims(decoder_target_data, -1)

# Print shapes
print("Encoder input shape:", input_tensor.shape)
print("Decoder input shape:", decoder_input_data.shape)
print("Decoder target shape:", decoder_target_data.shape)

model.fit(
    [input_tensor, decoder_input_data],
    decoder_target_data,
    batch_size=64,
    epochs=10,
    validation_split=0.2
)

# Define encoder model for inference
encoder_model = Model(encoder_inputs, encoder_states)

# Decoder setup
decoder_state_input_h = Input(shape=(units,))
decoder_state_input_c = Input(shape=(units,))
decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]

dec_emb2 = dec_emb_layer(decoder_inputs)
decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)
decoder_states2 = [state_h2, state_c2]
decoder_outputs2 = decoder_dense(decoder_outputs2)

decoder_model = Model(
    [decoder_inputs] + decoder_states_inputs,
    [decoder_outputs2] + decoder_states2
)

def translate(sentence):
    # Preprocess and tokenize the input sentence
    sentence = preprocess_sentence(sentence)
    sequence = inp_tokenizer.texts_to_sequences([sentence])
    sequence = pad_sequences(sequence, maxlen=input_tensor.shape[1], padding='post')

    # Encode the input sentence
    states_value = encoder_model.predict(sequence)

    # Create empty target sequence with only the <start> token
    target_seq = np.zeros((1, 1))
    target_seq[0, 0] = tgt_tokenizer.word_index['<start>']

    stop_condition = False
    decoded_sentence = ''

    while not stop_condition:
        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)

        # Get the word with highest probability
        sampled_token_index = np.argmax(output_tokens[0, -1, :])
        sampled_word = tgt_tokenizer.index_word.get(sampled_token_index, '')

        if (sampled_word == '<end>' or len(decoded_sentence.split()) > 20):
            stop_condition = True
        else:
            decoded_sentence += ' ' + sampled_word

        # Update target sequence and states
        target_seq = np.zeros((1, 1))
        target_seq[0, 0] = sampled_token_index
        states_value = [h, c]

    return decoded_sentence.strip()

print("English: I love you.")
print("French :", translate("I love you."))

print("English: How Are You.")
print("French :", translate("How Are You."))

print("English: What is your name.")
print("French :", translate("What is your name."))

# 1. Save model
model.save("translation_model.h5")

# 2. Save tokenizers
import pickle
with open("inp_tokenizer.pkl", "wb") as f:
    pickle.dump(inp_tokenizer, f)

with open("tgt_tokenizer.pkl", "wb") as f:
    pickle.dump(tgt_tokenizer, f)

# 3. Save config
with open("config.pkl", "wb") as f:
    pickle.dump((input_tensor.shape[1], units), f)

from google.colab import files

# Download one by one
files.download("translation_model.h5")
files.download("inp_tokenizer.pkl")
files.download("tgt_tokenizer.pkl")
files.download("config.pkl")

from google.colab import files
uploaded = files.upload()

import gradio as gr
import numpy as np
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.sequence import pad_sequences
import pickle

# Load assets
model = load_model("translation_model.h5")
with open("inp_tokenizer.pkl", "rb") as f:
    inp_tokenizer = pickle.load(f)
with open("tgt_tokenizer.pkl", "rb") as f:
    tgt_tokenizer = pickle.load(f)
with open("config.pkl", "rb") as f:
    max_input_len, units = pickle.load(f)

def preprocess_sentence(sent):
    return sent.lower().strip()

def translate(text):
    sentence = preprocess_sentence(text)
    sequence = inp_tokenizer.texts_to_sequences([sentence])
    sequence = pad_sequences(sequence, maxlen=max_input_len, padding='post')

    output_sentence = []
    for _ in range(max_input_len):
        prediction = model.predict([sequence])
        predicted_index = np.argmax(prediction[0, -1, :])
        predicted_word = tgt_tokenizer.index_word.get(predicted_index, '')

        if predicted_word == '' or predicted_word == '<end>':
            break

        output_sentence.append(predicted_word)

        # update sequence by removing first word and adding new word
        next_seq = tgt_tokenizer.texts_to_sequences([predicted_word])[0]
        next_seq = pad_sequences([next_seq], maxlen=1, padding='post')
        sequence = np.hstack([sequence[:, 1:], next_seq])

    return ' '.join(output_sentence) or "Unable to translate."


# Create Gradio app
iface = gr.Interface(
    fn=translate,
    inputs=gr.Textbox(lines=2, placeholder="Enter English sentence..."),
    outputs="text",
    title="English to French Translator",
    description="Translate English to French using LSTM model"
)

iface.launch(share=True)

import gradio as gr
import numpy as np
import pickle
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Load model and tokenizer files
model = load_model("translation_model.h5")

with open("inp_tokenizer.pkl", "rb") as f:
    inp_tokenizer = pickle.load(f)

with open("tgt_tokenizer.pkl", "rb") as f:
    tgt_tokenizer = pickle.load(f)

with open("config.pkl", "rb") as f:
    max_input_len, units = pickle.load(f)

# Preprocess function
def preprocess_sentence(sent):
    return sent.lower().strip()

# Translate function (single word prediction at each step)
def translate(text):
    try:
        sentence = text.lower().strip()
        sequence = inp_tokenizer.texts_to_sequences([sentence])
        sequence = pad_sequences(sequence, maxlen=max_input_len, padding='post')

        output_sentence = []
        for _ in range(max_input_len):
            prediction = model.predict([sequence])
            predicted_index = np.argmax(prediction[0, -1, :])
            predicted_word = tgt_tokenizer.index_word.get(predicted_index, '')

            if predicted_word == '' or predicted_word == '<end>':
                break

            output_sentence.append(predicted_word)
            next_seq = tgt_tokenizer.texts_to_sequences([predicted_word])[0]
            next_seq = pad_sequences([next_seq], maxlen=1, padding='post')
            sequence = np.hstack([sequence[:, 1:], next_seq])

        return ' '.join(output_sentence).strip() or "Translation failed."

    except Exception as e:
        return f"❌ Error: {str(e)}"


    return ' '.join(output_sentence).strip() or "Unable to translate."

# Gradio app
iface = gr.Interface(
    fn=translate,
    inputs=gr.Textbox(lines=2, placeholder="Enter English sentence..."),
    outputs="text",
    title="English to French Translator",
    description="Translate English to French using LSTM model"
)
print("Loaded model & tokenizers.")
print("Input vocab size:", len(inp_tokenizer.word_index))
print("Output vocab size:", len(tgt_tokenizer.word_index))
print("Max input length:", max_input_len)

iface.launch(share=True)

from tensorflow.keras.models import load_model, Model
from tensorflow.keras.layers import Input
import numpy as np
import pickle

# Load main model
model = load_model("translation_model.h5")

# Load tokenizers and config
with open("inp_tokenizer.pkl", "rb") as f:
    inp_tokenizer = pickle.load(f)
with open("tgt_tokenizer.pkl", "rb") as f:
    tgt_tokenizer = pickle.load(f)
with open("config.pkl", "rb") as f:
    max_input_len, units = pickle.load(f)

reverse_tgt_index = {i: w for w, i in tgt_tokenizer.word_index.items()}

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input

# Load encoder layers
encoder_inputs = model.input[0]  # Input for encoder
encoder_outputs, state_h_enc, state_c_enc = model.get_layer("lstm").output
encoder_states = [state_h_enc, state_c_enc]

# Encoder inference model
encoder_model = Model(encoder_inputs, encoder_states)

# Load decoder components
embedding_layer = model.get_layer("embedding_1")
embedding_dim = embedding_layer.output_dim  # get embedding output dim

# Create decoder inputs with 3D shape: (batch, time_step, embedding_dim)
decoder_inputs = Input(shape=(None, embedding_dim))

# Decoder state inputs
decoder_state_input_h = Input(shape=(units,))
decoder_state_input_c = Input(shape=(units,))
decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]

# Decoder layers
decoder_lstm = model.get_layer("lstm_1")
decoder_dense = model.get_layer("dense")

# Run decoder
decoder_outputs, state_h, state_c = decoder_lstm(
    decoder_inputs, initial_state=decoder_states_inputs)

decoder_states = [state_h, state_c]
decoder_outputs = decoder_dense(decoder_outputs)

# Decoder inference model
decoder_model = Model(
    [decoder_inputs] + decoder_states_inputs,
    [decoder_outputs] + decoder_states
)

def preprocess_sentence(sent):
    return sent.lower().strip()

def decode_sequence(input_seq):
    # Encode the input as state vectors
    states_value = encoder_model.predict(input_seq)

    # Start of sequence token
    target_seq = np.zeros((1, 1, embedding_dim))
    target_word_index = tgt_tokenizer.word_index.get("<start>", 1)

    # Embed the start token manually
    embedded_start = embedding_layer(np.array([[target_word_index]]))
    target_seq[:, 0, :] = embedded_start

    stop_condition = False
    decoded_sentence = ""

    while not stop_condition:
        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)

        sampled_token_index = np.argmax(output_tokens[0, -1, :])
        sampled_word = tgt_tokenizer.index_word.get(sampled_token_index, "")

        if sampled_word == "<end>" or len(decoded_sentence.split()) > max_input_len:
            stop_condition = True
        else:
            decoded_sentence += " " + sampled_word

        # Prepare next target input
        embedded_token = embedding_layer(np.array([[sampled_token_index]]))
        target_seq = np.zeros((1, 1, embedding_dim))
        target_seq[:, 0, :] = embedded_token

        # Update states
        states_value = [h, c]

    return decoded_sentence.strip()

def translate(text):
    try:
        sentence = preprocess_sentence(text)
        sequence = inp_tokenizer.texts_to_sequences([sentence])
        sequence = pad_sequences(sequence, maxlen=max_input_len, padding='post')
        return decode_sequence(sequence)
    except Exception as e:
        return f"❌ Error: {str(e)}"

import gradio as gr

iface = gr.Interface(
    fn=translate,
    inputs=gr.Textbox(lines=2, placeholder="Enter an English sentence..."),
    outputs="text",
    title="English to French Translator",
    description="Interactive translator built using Seq2Seq LSTM model trained on the ManyThings.org dataset"
)

iface.launch(share=True)
